{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting IMDS Using Satellite Embeddings with Optuna Hyperparameter Tuning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quarcs-lab/ds4bolivia/blob/master/notebooks/predict_imds_rf_optuna.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook extends our baseline IMDS prediction model by incorporating **Optuna**, a state-of-the-art hyperparameter optimization framework. While our baseline model used fixed hyperparameters, this notebook demonstrates how automated hyperparameter tuning can potentially improve model performance.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "1. **Hyperparameter Optimization Fundamentals**: Understanding why and how we tune model parameters\n",
    "2. **Optuna Framework**: Using Optuna for efficient Bayesian optimization\n",
    "3. **Study Design**: Creating optimization studies with objectives and search spaces\n",
    "4. **Visualization**: Interpreting optimization history and parameter importance\n",
    "5. **Best Practices**: Avoiding overfitting during hyperparameter search\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before starting this notebook, you should be familiar with:\n",
    "- Basic machine learning concepts (training, validation, testing)\n",
    "- Random Forest algorithms\n",
    "- Cross-validation techniques\n",
    "\n",
    "If you're new to these concepts, we recommend first completing the baseline notebook: [predict_imds_rf.ipynb](https://colab.research.google.com/github/quarcs-lab/ds4bolivia/blob/master/notebooks/predict_imds_rf.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Hyperparameter Tuning?\n",
    "\n",
    "### The Problem with Default Parameters\n",
    "\n",
    "Machine learning models have two types of parameters:\n",
    "\n",
    "1. **Model Parameters**: Learned from data during training (e.g., tree splits in Random Forest)\n",
    "2. **Hyperparameters**: Set before training and control the learning process (e.g., number of trees, max depth)\n",
    "\n",
    "Default hyperparameters are designed to work \"reasonably well\" across many datasets, but they're rarely optimal for your specific problem. Hyperparameter tuning finds the best configuration for your data.\n",
    "\n",
    "### Why Optuna?\n",
    "\n",
    "Traditional approaches like Grid Search test every combination (slow) or Random Search samples randomly (inefficient). **Optuna** uses **Bayesian optimization** which:\n",
    "\n",
    "- **Learns from previous trials**: Uses past results to suggest promising hyperparameters\n",
    "- **Prunes unpromising trials**: Stops bad configurations early, saving computation\n",
    "- **Handles complex search spaces**: Works with continuous, discrete, and conditional parameters\n",
    "\n",
    "```\n",
    "Grid Search:     [■][■][■][■][■][■][■][■][■]  → Tests ALL combinations\n",
    "Random Search:   [■][ ][■][ ][ ][■][ ][■][ ]  → Random sampling\n",
    "Optuna:          [■]→[■]→→→[■]→→→→→→[★]       → Smart, directed search\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we install Optuna and import all required libraries. In Google Colab, Optuna is not pre-installed, so we need to install it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALL OPTUNA\n",
    "# =============================================================================\n",
    "# Optuna is not included in the default Colab environment, so we install it.\n",
    "# The -q flag suppresses verbose output for cleaner notebooks.\n",
    "\n",
    "!pip install -q optuna\n",
    "\n",
    "print(\"Optuna installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning - scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,    # Split data into train/test sets\n",
    "    cross_val_score,     # Perform cross-validation\n",
    "    KFold                # K-Fold cross-validation splitter\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # MSE metric\n",
    "    mean_absolute_error, # MAE metric\n",
    "    r2_score             # R² coefficient of determination\n",
    ")\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,      # Track optimization progress\n",
    "    plot_param_importances,         # Which hyperparameters matter most\n",
    "    plot_contour,                   # 2D parameter relationships\n",
    "    plot_slice                      # 1D parameter effects\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optuna logging - set to WARNING to reduce verbosity during optimization\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY\n",
    "# =============================================================================\n",
    "# Setting random seeds ensures that results are reproducible.\n",
    "# This is crucial for scientific work and debugging.\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "We load the same datasets as in the baseline notebook:\n",
    "- **SDG Data**: Contains IMDS (our target variable)\n",
    "- **Satellite Embeddings**: 64 features derived from satellite imagery\n",
    "- **Region Names**: Municipality names for interpretation\n",
    "\n",
    "### About the IMDS\n",
    "\n",
    "The **IMDS (Índice Municipal de Desarrollo Sostenible)** is a composite index that aggregates all SDG indicators into a single score (0-100). It represents overall sustainable development at the municipal level in Bolivia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "# We load data directly from the DS4Bolivia GitHub repository.\n",
    "# This ensures reproducibility - anyone can run this notebook.\n",
    "\n",
    "REPO_URL = \"https://raw.githubusercontent.com/quarcs-lab/ds4bolivia/master\"\n",
    "\n",
    "# Define data URLs\n",
    "url_sdg = f\"{REPO_URL}/sdg/sdg.csv\"                                    # SDG indices (target)\n",
    "url_emb = f\"{REPO_URL}/satelliteEmbeddings/satelliteEmbeddings2017.csv\" # Satellite features\n",
    "url_names = f\"{REPO_URL}/regionNames/regionNames.csv\"                   # Municipality names\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets from GitHub...\")\n",
    "df_sdg = pd.read_csv(url_sdg)\n",
    "df_embeddings = pd.read_csv(url_emb)\n",
    "df_names = pd.read_csv(url_names)\n",
    "\n",
    "print(f\"✓ SDG data: {len(df_sdg)} municipalities\")\n",
    "print(f\"✓ Satellite embeddings: {len(df_embeddings)} municipalities, {len(df_embeddings.columns)-1} features\")\n",
    "print(f\"✓ Region names: {len(df_names)} municipalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA MERGING\n",
    "# =============================================================================\n",
    "# We merge all datasets using 'asdf_id' as the common key.\n",
    "# This creates a single dataframe with target, features, and names.\n",
    "\n",
    "# Step 1: Merge SDG data (with IMDS target) with satellite embeddings\n",
    "df_merged = df_sdg[['asdf_id', 'imds']].merge(\n",
    "    df_embeddings,\n",
    "    on='asdf_id',\n",
    "    how='inner'  # Keep only municipalities present in both datasets\n",
    ")\n",
    "\n",
    "# Step 2: Add municipality and department names for interpretation\n",
    "df_merged = df_merged.merge(\n",
    "    df_names[['asdf_id', 'mun', 'dep']],\n",
    "    on='asdf_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 3: Remove any rows with missing IMDS values\n",
    "df_clean = df_merged.dropna(subset=['imds']).copy()\n",
    "\n",
    "print(f\"Merged dataset: {len(df_clean)} municipalities\")\n",
    "print(f\"Columns: {len(df_clean.columns)} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE AND TARGET PREPARATION\n",
    "# =============================================================================\n",
    "# We separate our data into:\n",
    "# - X (features): The 64 satellite embedding dimensions (A00 to A63)\n",
    "# - y (target): The IMDS score we want to predict\n",
    "\n",
    "# Create list of feature column names (A00, A01, ..., A63)\n",
    "embedding_cols = [f'A{str(i).zfill(2)}' for i in range(64)]\n",
    "\n",
    "# Extract feature matrix and target vector\n",
    "X = df_clean[embedding_cols].values  # Shape: (n_samples, 64)\n",
    "y = df_clean['imds'].values          # Shape: (n_samples,)\n",
    "\n",
    "print(\"Data prepared for modeling:\")\n",
    "print(f\"  Features (X): {X.shape[0]} samples × {X.shape[1]} features\")\n",
    "print(f\"  Target (y): {y.shape[0]} samples\")\n",
    "print(f\"\\nIMDS Statistics:\")\n",
    "print(f\"  Range: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "print(f\"  Mean ± Std: {y.mean():.2f} ± {y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Validation-Test Split Strategy\n",
    "\n",
    "### Why Three Sets?\n",
    "\n",
    "When doing hyperparameter tuning, we need to be careful about data leakage:\n",
    "\n",
    "| Set | Purpose | Used During |\n",
    "|-----|---------|-------------|\n",
    "| **Training** | Fit model parameters | Each trial |\n",
    "| **Validation** | Evaluate hyperparameters | Optimization |\n",
    "| **Test** | Final unbiased evaluation | Only once, at the end |\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Full Dataset (339)                       │\n",
    "├─────────────────────────────────────────────┬───────────────┤\n",
    "│           Training + Validation (271)       │   Test (68)   │\n",
    "│  ┌─────────────────────┬──────────────────┐ │               │\n",
    "│  │ Train (CV Folds)    │ Val (CV Folds)   │ │  Final Eval   │\n",
    "│  │ Used by Optuna      │ Used by Optuna   │ │  Held Out     │\n",
    "│  └─────────────────────┴──────────────────┘ │               │\n",
    "└─────────────────────────────────────────────┴───────────────┘\n",
    "```\n",
    "\n",
    "**Important**: The test set is NEVER used during hyperparameter tuning. It provides an unbiased estimate of final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN-TEST SPLIT\n",
    "# =============================================================================\n",
    "# We hold out 20% of the data as a final test set.\n",
    "# The remaining 80% will be used for training and cross-validation.\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test, idx_train_val, idx_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    df_clean.index,  # Keep track of original indices for later analysis\n",
    "    test_size=0.2,   # 20% for final testing\n",
    "    random_state=RANDOM_STATE  # Reproducibility\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(f\"  Training + Validation: {len(X_train_val)} municipalities ({len(X_train_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test (held out):       {len(X_test)} municipalities ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nNote: Optuna will use cross-validation on the training set.\")\n",
    "print(f\"      The test set remains untouched until final evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Optuna: Core Concepts\n",
    "\n",
    "Before we define our optimization, let's understand Optuna's key concepts:\n",
    "\n",
    "### Key Terminology\n",
    "\n",
    "| Term | Definition | Example |\n",
    "|------|------------|--------|\n",
    "| **Study** | The optimization task | \"Find best Random Forest hyperparameters\" |\n",
    "| **Trial** | One evaluation with specific hyperparameters | n_estimators=100, max_depth=10 |\n",
    "| **Objective** | Function to optimize | Cross-validation R² score |\n",
    "| **Search Space** | Range of possible values | n_estimators ∈ [50, 500] |\n",
    "| **Sampler** | Algorithm to suggest next trial | TPE (Tree-structured Parzen Estimator) |\n",
    "| **Pruner** | Stops unpromising trials early | MedianPruner |\n",
    "\n",
    "### How Optuna Works\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  For each trial:                                            │\n",
    "│                                                             │\n",
    "│  1. Sampler suggests hyperparameters based on history       │\n",
    "│  2. Objective function trains model with these parameters   │\n",
    "│  3. Cross-validation score is computed                      │\n",
    "│  4. Result is recorded for future sampling                  │\n",
    "│  5. Repeat until n_trials completed                         │\n",
    "│                                                             │\n",
    "│  Trial 1 → Trial 2 → Trial 3 → ... → Best Parameters       │\n",
    "│    ↓         ↓         ↓                                    │\n",
    "│  Learn    Learn     Learn (gets smarter over time)          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Defining the Objective Function\n",
    "\n",
    "The **objective function** is the heart of Optuna optimization. It:\n",
    "1. Receives a `trial` object from Optuna\n",
    "2. Samples hyperparameters from defined search spaces\n",
    "3. Trains a model with those hyperparameters\n",
    "4. Returns a score to optimize (we maximize cross-validation R²)\n",
    "\n",
    "### Search Space Design\n",
    "\n",
    "Choosing good search spaces is crucial. We consider:\n",
    "- **Physical constraints**: max_depth can't be negative\n",
    "- **Computational limits**: Too many trees = slow training\n",
    "- **Domain knowledge**: Typical ranges that work well\n",
    "\n",
    "| Hyperparameter | Search Space | Rationale |\n",
    "|----------------|--------------|----------|\n",
    "| `n_estimators` | 50 - 500 | More trees = better but slower |\n",
    "| `max_depth` | 5 - 50 | Too shallow = underfit, too deep = overfit |\n",
    "| `min_samples_split` | 2 - 20 | Controls tree complexity |\n",
    "| `min_samples_leaf` | 1 - 10 | Prevents tiny leaf nodes |\n",
    "| `max_features` | sqrt, log2, None | Features per split |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OBJECTIVE FUNCTION DEFINITION\n",
    "# =============================================================================\n",
    "# This function is called by Optuna for each trial.\n",
    "# It samples hyperparameters, trains a model, and returns the CV score.\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for Random Forest hyperparameter optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object that suggests hyperparameter values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Mean cross-validation R² score (to be maximized)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Sample hyperparameters from search spaces\n",
    "    # -------------------------------------------------------------------------\n",
    "    # trial.suggest_* methods define the search space and sample values\n",
    "    \n",
    "    # Number of trees in the forest\n",
    "    # More trees generally improve performance but increase training time\n",
    "    n_estimators = trial.suggest_int(\n",
    "        'n_estimators',  # Parameter name (used in results)\n",
    "        50,              # Minimum value\n",
    "        500,             # Maximum value\n",
    "        step=50          # Step size (50, 100, 150, ...)\n",
    "    )\n",
    "    \n",
    "    # Maximum depth of each tree\n",
    "    # Deeper trees can capture complex patterns but may overfit\n",
    "    max_depth = trial.suggest_int(\n",
    "        'max_depth',\n",
    "        5,               # Minimum (shallow trees)\n",
    "        50               # Maximum (deep trees)\n",
    "    )\n",
    "    \n",
    "    # Minimum samples required to split an internal node\n",
    "    # Higher values prevent overfitting by requiring more evidence for splits\n",
    "    min_samples_split = trial.suggest_int(\n",
    "        'min_samples_split',\n",
    "        2,               # scikit-learn default\n",
    "        20               # More conservative\n",
    "    )\n",
    "    \n",
    "    # Minimum samples required at each leaf node\n",
    "    # Prevents trees from creating leaves with very few samples\n",
    "    min_samples_leaf = trial.suggest_int(\n",
    "        'min_samples_leaf',\n",
    "        1,               # scikit-learn default\n",
    "        10               # More conservative\n",
    "    )\n",
    "    \n",
    "    # Number of features to consider at each split\n",
    "    # 'sqrt' and 'log2' add randomness, reducing correlation between trees\n",
    "    max_features = trial.suggest_categorical(\n",
    "        'max_features',\n",
    "        ['sqrt', 'log2', None]  # None = use all features\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Create and train the model\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=RANDOM_STATE,  # Reproducibility\n",
    "        n_jobs=-1                   # Use all CPU cores\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Evaluate using cross-validation\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We use 5-fold CV to get a robust estimate of model performance.\n",
    "    # This helps avoid overfitting to a single validation split.\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model,\n",
    "        X_train_val,     # Training data (not test set!)\n",
    "        y_train_val,\n",
    "        cv=cv,\n",
    "        scoring='r2'     # R² score (coefficient of determination)\n",
    "    )\n",
    "    \n",
    "    # Return mean CV score - Optuna will try to MAXIMIZE this\n",
    "    return cv_scores.mean()\n",
    "\n",
    "print(\"Objective function defined successfully!\")\n",
    "print(\"\\nSearch space summary:\")\n",
    "print(\"  n_estimators:      [50, 500] step=50\")\n",
    "print(\"  max_depth:         [5, 50]\")\n",
    "print(\"  min_samples_split: [2, 20]\")\n",
    "print(\"  min_samples_leaf:  [1, 10]\")\n",
    "print(\"  max_features:      ['sqrt', 'log2', None]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Optimization Study\n",
    "\n",
    "Now we create an Optuna **study** and run the optimization. Key settings:\n",
    "\n",
    "- **`direction='maximize'`**: We want to maximize R² (higher is better)\n",
    "- **`n_trials=100`**: Number of hyperparameter combinations to try\n",
    "- **`sampler=TPESampler`**: Tree-structured Parzen Estimator (Bayesian optimization)\n",
    "\n",
    "### What Happens During Optimization?\n",
    "\n",
    "1. Optuna suggests hyperparameters based on previous trials\n",
    "2. The objective function trains a model and computes CV R²\n",
    "3. Optuna records the result and updates its model\n",
    "4. After all trials, we get the best hyperparameters found\n",
    "\n",
    "**Note**: This may take several minutes depending on your hardware. With 100 trials and 5-fold CV, we're training 500 Random Forest models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE AND RUN OPTUNA STUDY\n",
    "# =============================================================================\n",
    "\n",
    "# Number of optimization trials\n",
    "# More trials = better chance of finding optimal parameters, but takes longer\n",
    "N_TRIALS = 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Number of trials: {N_TRIALS}\")\n",
    "print(f\"  CV folds per trial: 5\")\n",
    "print(f\"  Total models to train: {N_TRIALS * 5}\")\n",
    "print(f\"  Objective: Maximize CV R²\")\n",
    "print(f\"\\nThis may take several minutes...\\n\")\n",
    "\n",
    "# Create the study\n",
    "# - direction='maximize': We want higher R² scores\n",
    "# - sampler: TPE is the default and works well for most problems\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "    study_name='IMDS_RandomForest_Optimization'\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "# show_progress_bar=True gives visual feedback during optimization\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=N_TRIALS,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Optimization Results\n",
    "\n",
    "After optimization, we analyze:\n",
    "1. **Best hyperparameters found**\n",
    "2. **Optimization history**: How scores improved over trials\n",
    "3. **Parameter importance**: Which hyperparameters matter most\n",
    "\n",
    "This analysis helps us understand not just WHAT works, but WHY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BEST TRIAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Get the best trial from the study\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nBest CV R² Score: {best_trial.value:.4f}\")\n",
    "print(f\"Trial Number: {best_trial.number}\")\n",
    "print(f\"\\nOptimal Hyperparameters:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Store best parameters for later use\n",
    "best_params = best_trial.params\n",
    "\n",
    "for param_name, param_value in best_params.items():\n",
    "    print(f\"  {param_name:20s}: {param_value}\")\n",
    "\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OPTIMIZATION HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "# This shows how the optimization progressed over trials.\n",
    "# The blue line shows best value found so far (should increase over time).\n",
    "\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(\n",
    "    title=\"Optimization History: CV R² Score Over Trials\",\n",
    "    xaxis_title=\"Trial Number\",\n",
    "    yaxis_title=\"CV R² Score\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER IMPORTANCE\n",
    "# =============================================================================\n",
    "# This shows which hyperparameters have the most impact on model performance.\n",
    "# High importance = this parameter significantly affects the score.\n",
    "\n",
    "fig = plot_param_importances(study)\n",
    "fig.update_layout(\n",
    "    title=\"Hyperparameter Importance Analysis\",\n",
    "    xaxis_title=\"Importance for CV R² Score\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETER SLICE PLOTS\n",
    "# =============================================================================\n",
    "# These plots show how the objective value changes with each parameter.\n",
    "# Helps identify optimal ranges and parameter sensitivity.\n",
    "\n",
    "fig = plot_slice(study)\n",
    "fig.update_layout(title=\"Parameter Effects on CV R² Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOP 10 TRIALS SUMMARY\n",
    "# =============================================================================\n",
    "# Looking at the top trials helps understand the \"good\" parameter region.\n",
    "\n",
    "# Get trials sorted by value (descending)\n",
    "trials_df = study.trials_dataframe()\n",
    "trials_df = trials_df.sort_values('value', ascending=False)\n",
    "\n",
    "print(\"TOP 10 TRIALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select relevant columns\n",
    "display_cols = ['number', 'value'] + [f'params_{p}' for p in best_params.keys()]\n",
    "top_10 = trials_df[display_cols].head(10)\n",
    "\n",
    "# Rename columns for clarity\n",
    "top_10.columns = ['Trial', 'CV R²'] + list(best_params.keys())\n",
    "\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the Final Model with Optimal Hyperparameters\n",
    "\n",
    "Now we train the final model using:\n",
    "1. **Best hyperparameters** found by Optuna\n",
    "2. **Full training data** (all 271 municipalities in train_val set)\n",
    "\n",
    "This model will be evaluated on the held-out test set to get an unbiased performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAIN FINAL MODEL WITH BEST HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training final model with optimal hyperparameters...\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Create final model with best parameters\n",
    "final_model = RandomForestRegressor(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train on full training set\n",
    "final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "print(\"✓ Final model trained successfully!\")\n",
    "print(f\"\\nModel configuration:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation on Held-Out Test Set\n",
    "\n",
    "This is the moment of truth! We evaluate our tuned model on the test set that was **never used during optimization**. This gives us an unbiased estimate of how well our model will perform on new, unseen data.\n",
    "\n",
    "### Metrics We Use\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **R²** | 1 - (SS_res / SS_tot) | % of variance explained (0-1, higher is better) |\n",
    "| **RMSE** | √(mean((y - ŷ)²)) | Average error magnitude (same units as target) |\n",
    "| **MAE** | mean(\\|y - ŷ\\|) | Average absolute error (robust to outliers) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREDICTIONS AND METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# Make predictions on both sets\n",
    "y_train_pred = final_model.predict(X_train_val)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_r2 = r2_score(y_train_val, y_train_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_val, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train_val, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set (THE IMPORTANT ONES)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Calculate prediction errors for later analysis\n",
    "test_errors = y_test - y_test_pred\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<15} {'Training Set':<20} {'Test Set':<20}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'R² Score':<15} {train_r2:<20.4f} {test_r2:<20.4f}\")\n",
    "print(f\"{'RMSE':<15} {train_rmse:<20.4f} {test_rmse:<20.4f}\")\n",
    "print(f\"{'MAE':<15} {train_mae:<20.4f} {test_mae:<20.4f}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"\\nOptuna CV R² (during optimization): {best_trial.value:.4f}\")\n",
    "print(f\"Test R² (final, unbiased):          {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREDICTION VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "ax1.set_xlabel('Actual IMDS', fontsize=11)\n",
    "ax1.set_ylabel('Predicted IMDS', fontsize=11)\n",
    "ax1.set_title(f'Actual vs Predicted\\nTest R² = {test_r2:.4f}', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_test_pred, test_errors, alpha=0.6, edgecolors='k', linewidth=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted IMDS', fontsize=11)\n",
    "ax2.set_ylabel('Residuals (Actual - Predicted)', fontsize=11)\n",
    "ax2.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Error Distribution\n",
    "ax3 = axes[2]\n",
    "ax3.hist(test_errors, bins=15, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax3.axvline(x=test_errors.mean(), color='blue', linestyle='-', lw=2, \n",
    "            label=f'Mean: {test_errors.mean():.2f}')\n",
    "ax3.set_xlabel('Prediction Error', fontsize=11)\n",
    "ax3.set_ylabel('Frequency', fontsize=11)\n",
    "ax3.set_title(f'Error Distribution\\nMAE = {test_mae:.2f}', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis\n",
    "\n",
    "Random Forest provides **feature importances** based on how much each feature contributes to reducing prediction error across all trees. This helps us understand which satellite embedding dimensions are most informative for predicting IMDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "importances = final_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "\n",
    "# Calculate cumulative importance\n",
    "cumsum = np.cumsum(importances[indices])\n",
    "n_features_80 = np.argmax(cumsum >= 0.80) + 1\n",
    "\n",
    "print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Rank':<6} {'Feature':<10} {'Importance':<12} {'Cumulative'}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "cumulative = 0\n",
    "for i in range(15):\n",
    "    idx = indices[i]\n",
    "    cumulative += importances[idx]\n",
    "    print(f\"{i+1:<6} {embedding_cols[idx]:<10} {importances[idx]:.4f}       {cumulative*100:.1f}%\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nFeatures needed for 80% importance: {n_features_80}/64 ({n_features_80/64*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE IMPORTANCE VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Top 20 Feature Importances\n",
    "ax1 = axes[0]\n",
    "top_n = 20\n",
    "top_indices = indices[:top_n]\n",
    "top_features = [embedding_cols[i] for i in top_indices]\n",
    "top_importances = importances[top_indices]\n",
    "\n",
    "ax1.barh(range(top_n), top_importances, edgecolor='black', color='steelblue')\n",
    "ax1.set_yticks(range(top_n))\n",
    "ax1.set_yticklabels(top_features)\n",
    "ax1.set_xlabel('Importance', fontsize=11)\n",
    "ax1.set_title('Top 20 Feature Importances', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Cumulative Importance\n",
    "ax2 = axes[1]\n",
    "ax2.plot(range(1, 65), cumsum, linewidth=2, marker='o', markersize=3, color='steelblue')\n",
    "ax2.axhline(y=0.8, color='r', linestyle='--', lw=2, label='80% threshold')\n",
    "ax2.axvline(x=n_features_80, color='g', linestyle='--', lw=2, label=f'{n_features_80} features')\n",
    "ax2.fill_between(range(1, n_features_80+1), cumsum[:n_features_80], alpha=0.3)\n",
    "ax2.set_xlabel('Number of Features', fontsize=11)\n",
    "ax2.set_ylabel('Cumulative Importance', fontsize=11)\n",
    "ax2.set_title('Cumulative Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(1, 64)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Error Analysis\n",
    "\n",
    "Understanding where the model makes large errors helps identify:\n",
    "- **Systematic biases** (e.g., urban vs rural)\n",
    "- **Challenging cases** that may need additional features\n",
    "- **Potential data quality issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE RESULTS DATAFRAME\n",
    "# =============================================================================\n",
    "\n",
    "# Create dataframe with test set results\n",
    "test_results = df_clean.iloc[idx_test].copy()\n",
    "test_results['imds_actual'] = y_test\n",
    "test_results['imds_predicted'] = y_test_pred\n",
    "test_results['error'] = test_errors\n",
    "test_results['abs_error'] = np.abs(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# OVERPREDICTED MUNICIPALITIES\n",
    "# =============================================================================\n",
    "# These municipalities have LOWER actual IMDS than the model predicts.\n",
    "# The model thinks they should be more developed based on satellite features.\n",
    "\n",
    "overpredicted = test_results.nsmallest(10, 'error')\n",
    "\n",
    "print(\"TOP 10 OVERPREDICTED MUNICIPALITIES\")\n",
    "print(\"(Model predicts HIGHER development than actual)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for _, row in overpredicted.iterrows():\n",
    "    print(f\"\\n  {row['mun']}, {row['dep']}\")\n",
    "    print(f\"    Actual: {row['imds_actual']:.2f} | Predicted: {row['imds_predicted']:.2f} | Error: {row['error']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UNDERPREDICTED MUNICIPALITIES\n",
    "# =============================================================================\n",
    "# These municipalities have HIGHER actual IMDS than the model predicts.\n",
    "# They achieve better development than their satellite features suggest.\n",
    "\n",
    "underpredicted = test_results.nlargest(10, 'error')\n",
    "\n",
    "print(\"TOP 10 UNDERPREDICTED MUNICIPALITIES\")\n",
    "print(\"(Model predicts LOWER development than actual)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for _, row in underpredicted.iterrows():\n",
    "    print(f\"\\n  {row['mun']}, {row['dep']}\")\n",
    "    print(f\"    Actual: {row['imds_actual']:.2f} | Predicted: {row['imds_predicted']:.2f} | Error: {row['error']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ERROR ANALYSIS BY DEPARTMENT\n",
    "# =============================================================================\n",
    "\n",
    "dept_errors = test_results.groupby('dep').agg({\n",
    "    'error': ['mean', 'std', 'count'],\n",
    "    'abs_error': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "dept_errors.columns = ['Mean Error', 'Std Error', 'Count', 'Mean Abs Error']\n",
    "dept_errors = dept_errors.sort_values('Mean Error')\n",
    "\n",
    "print(\"\\nPREDICTION ERRORS BY DEPARTMENT\")\n",
    "print(\"=\"*60)\n",
    "print(dept_errors.to_string())\n",
    "print(\"\\nPositive error = Underpredicted (actual > predicted)\")\n",
    "print(\"Negative error = Overpredicted (actual < predicted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison: Baseline vs Optuna-Tuned Model\n",
    "\n",
    "Let's compare our Optuna-tuned model with the baseline model that used fixed hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "# Baseline model parameters (from predict_imds_rf.ipynb)\n",
    "baseline_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features': 'sqrt'\n",
    "}\n",
    "\n",
    "# Train baseline model for comparison\n",
    "baseline_model = RandomForestRegressor(\n",
    "    **baseline_params,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "baseline_model.fit(X_train_val, y_train_val)\n",
    "baseline_pred = baseline_model.predict(X_test)\n",
    "baseline_r2 = r2_score(y_test, baseline_pred)\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "\n",
    "print(\"MODEL COMPARISON: BASELINE vs OPTUNA-TUNED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<20} {'Baseline':<20} {'Optuna-Tuned':<20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Test R²':<20} {baseline_r2:<20.4f} {test_r2:<20.4f}\")\n",
    "print(f\"{'Test MAE':<20} {baseline_mae:<20.4f} {test_mae:<20.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Calculate improvement\n",
    "r2_improvement = (test_r2 - baseline_r2) / baseline_r2 * 100 if baseline_r2 != 0 else 0\n",
    "mae_improvement = (baseline_mae - test_mae) / baseline_mae * 100\n",
    "\n",
    "print(f\"\\nPerformance Change:\")\n",
    "print(f\"  R² change:  {r2_improvement:+.1f}%\")\n",
    "print(f\"  MAE change: {mae_improvement:+.1f}% (negative = improvement)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYPERPARAMETER COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nHYPERPARAMETER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Parameter':<20} {'Baseline':<20} {'Optuna-Tuned':<20}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for param in baseline_params.keys():\n",
    "    baseline_val = baseline_params[param]\n",
    "    optuna_val = best_params[param]\n",
    "    print(f\"{param:<20} {str(baseline_val):<20} {str(optuna_val):<20}\")\n",
    "\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Hyperparameter tuning with Optuna** provides a systematic, efficient way to find optimal model configurations\n",
    "\n",
    "2. **Key hyperparameters** identified through importance analysis show which parameters have the most impact on model performance\n",
    "\n",
    "3. **Prediction patterns** reveal systematic biases (urban vs rural) that satellite features don't fully capture\n",
    "\n",
    "### Limitations to Consider\n",
    "\n",
    "- **IMDS is a composite index** combining many dimensions not directly observable from satellite imagery\n",
    "- **Sample size (339 municipalities)** limits the complexity of models we can train\n",
    "- **Hyperparameter tuning can't overcome fundamental feature limitations** - we need the right features first\n",
    "\n",
    "### Next Steps for Further Improvement\n",
    "\n",
    "1. **Feature engineering**: Create new features from existing embeddings (ratios, PCA components)\n",
    "2. **Additional data sources**: Incorporate night-time lights, land cover, or other satellite products\n",
    "3. **Ensemble methods**: Combine multiple models (XGBoost, LightGBM, Neural Networks)\n",
    "4. **Spatial features**: Include geographic coordinates or neighbor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY: IMDS PREDICTION WITH OPTUNA HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "OPTIMIZATION DETAILS:\n",
    "  • Trials completed: {N_TRIALS}\n",
    "  • Best CV R² found: {best_trial.value:.4f}\n",
    "  • Optimization method: TPE (Tree-structured Parzen Estimator)\n",
    "\n",
    "BEST HYPERPARAMETERS:\n",
    "  • n_estimators: {best_params['n_estimators']}\n",
    "  • max_depth: {best_params['max_depth']}\n",
    "  • min_samples_split: {best_params['min_samples_split']}\n",
    "  • min_samples_leaf: {best_params['min_samples_leaf']}\n",
    "  • max_features: {best_params['max_features']}\n",
    "\n",
    "FINAL MODEL PERFORMANCE:\n",
    "  • Test R² Score: {test_r2:.4f}\n",
    "  • Test RMSE: {test_rmse:.4f} IMDS points\n",
    "  • Test MAE: {test_mae:.4f} IMDS points\n",
    "\n",
    "FEATURE IMPORTANCE:\n",
    "  • Features for 80% importance: {n_features_80}/64 ({n_features_80/64*100:.1f}%)\n",
    "  • Top 3 features: {', '.join([embedding_cols[i] for i in indices[:3]])}\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises for Students\n",
    "\n",
    "### Exercise 1: Modify the Search Space\n",
    "Try expanding or narrowing the search space for `n_estimators` and `max_depth`. How does this affect the optimization results?\n",
    "\n",
    "### Exercise 2: Different Samplers\n",
    "Replace `TPESampler` with `RandomSampler` and compare the optimization efficiency. How many trials does each take to find good parameters?\n",
    "\n",
    "### Exercise 3: Add Pruning\n",
    "Implement early stopping (pruning) to terminate unpromising trials. Use `optuna.pruners.MedianPruner` and integrate it with cross-validation.\n",
    "\n",
    "### Exercise 4: Multi-Objective Optimization\n",
    "Modify the objective to optimize both R² (maximize) and training time (minimize). This is a multi-objective optimization problem.\n",
    "\n",
    "### Exercise 5: Different Models\n",
    "Apply the same Optuna framework to a Gradient Boosting model (`GradientBoostingRegressor` or `XGBRegressor`). Compare results with Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **Optuna Documentation**: https://optuna.readthedocs.io/\n",
    "- **Akiba et al. (2019)**: Optuna: A Next-generation Hyperparameter Optimization Framework. KDD 2019.\n",
    "- **DS4Bolivia Repository**: https://github.com/quarcs-lab/ds4bolivia\n",
    "- **scikit-learn Random Forest**: https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
